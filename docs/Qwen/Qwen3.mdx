# Qwen3

## 1. Model Introduction

[Qwen3 series](https://github.com/QwenLM/Qwen3) are the most powerful vision-language models in the Qwen series to date, featuring advanced capabilities in multi-modal understanding, reasoning, and agentic applications.

This generation delivers comprehensive upgrades across the board:

- **Stronger general intelligence**: Significant improvements in instruction following, logical reasoning, text comprehension, mathematics, science, coding, and tool usage.
- **Broader multilingual knowledge**: Substantial gains in long-tail knowledge coverage across multiple languages.
- **More helpful & aligned responses**: Markedly better alignment with user preferences in subjective and open-ended tasks, enabling higher-quality, more useful text generation.
- **Extended context length**:  Enhanced capabilities in understanding and reasoning over 256K-token long contexts.
- **Stronger agent interaction capabilities**: Improved tool use and search-based agent performance
- **Flexible deployment options**: Available in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning-enhanced Thinking editions

For more details, please refer to the [official Qwen3 GitHub Repository](https://github.com/QwenLM/Qwen3).

## 2. SGLang Installation

SGLang offers multiple installation methods. You can choose the most suitable installation method based on your hardware platform and requirements.

Please refer to the [official SGLang installation guide](https://docs.sglang.ai/get_started/install.html) for installation instructions.

## 3. Model Deployment

This section provides deployment configurations optimized for different hardware platforms and use cases.

### 3.1 Basic Configuration

The Qwen3 series offers models in various sizes and architectures, optimized for different hardware platforms. The recommended launch configurations vary by hardware and model size.

**Interactive Command Generator**: Use the configuration selector below to automatically generate the appropriate deployment command for your hardware platform, model size, quantization method, and thinking capabilities.

import Qwen3ConfigGenerator from '@site/src/components/Qwen3ConfigGenerator';

<Qwen3ConfigGenerator />

{/* Old inline script configuration has been replaced with ConfigGenerator component */}
{/* const CONFIG = {
    modelFamily: 'Qwen',
  
    options: {
        hardware: {
            name: 'hardware',
            title: 'Hardware Platform',
            items: [
        { id: 'b200', label: 'B200', default: true },
        { id: 'h100', label: 'H100', default: false },
        { id: 'h200', label: 'H200', default: false }
            ]
        },
        modelsize: {
            name: 'modelsize',
            title: 'Model Size',
            items: [
        { id: '235b', label: '235B', subtitle: 'MOE', default: true },
        { id: '30b', label: '30B', subtitle: 'MOE', default: false },
        { id: '32b', label: '32B', subtitle: 'Dense', default: false },
        { id: '14b', label: '14B', subtitle: 'Dense', default: false },
        { id: '8b', label: '8B', subtitle: 'Dense', default: false },
        { id: '4b', label: '4B', subtitle: 'Dense', default: false },
        { id: '1.7b', label: '1.7B', subtitle: 'Dense', default: false },
        { id: '0.6b', label: '0.6B', subtitle: 'Dense', default: false }
            ]
        },
        quantization: {
            name: 'quantization',
            title: 'Quantization',
            items: [
        { id: 'bf16', label: 'BF16', default: true },
        { id: 'fp8', label: 'FP8', default: false }
            ]
        },
        thinking: {
            name: 'thinking',
            title: 'Thinking Capabilities',
            items: [
        { id: 'instruct', label: 'Instruct', default: true },
        { id: 'thinking', label: 'Thinking', default: false }
    ],
            commandRule: (value) => value === 'thinking' ? '--reasoning-parser qwen3' : null
        },
        toolcall: {
            name: 'toolcall',
            title: 'Tool Call Parser',
            items: [
                { id: 'disabled', label: 'Disabled', default: true },
                { id: 'enabled', label: 'Enabled', default: false }
            ],
            commandRule: (value) => value === 'enabled' ? '--tool-call-parser qwen' : null
        }
    },
  
    modelConfigs: {
        '235b': {
            baseName: '235B-A22B',
            isMOE: true,
            h100: { tp: 8, ep: 0, bf16: true, fp8: true },
            h200: { tp: 8, ep: 0, bf16: true, fp8: true },
            b200: { tp: 8, ep: 0, bf16: true, fp8: true }
        },
        '30b': {
            baseName: '30B-A3B',
            isMOE: true,
            h100: { tp: 1, ep: 0, bf16: true, fp8: true },
            h200: { tp: 1, ep: 0, bf16: true, fp8: true },
            b200: { tp: 1, ep: 0, bf16: true, fp8: true }
        },
        '32b': {
            baseName: '32B',
            isMOE: false,
            h100: { tp: 1, ep: 0, bf16: true, fp8: true },
            h200: { tp: 1, ep: 0, bf16: true, fp8: true },
            b200: { tp: 1, ep: 0, bf16: true, fp8: true }
        },
        '14b': {
            baseName: '14B',
            isMOE: false,
            h100: { tp: 1, ep: 0, bf16: true, fp8: true },
            h200: { tp: 1, ep: 0, bf16: true, fp8: true },
            b200: { tp: 1, ep: 0, bf16: true, fp8: true }
        },
        '8b': {
            baseName: '8B',
            isMOE: false,
            h100: { tp: 1, ep: 0, bf16: true, fp8: true },
            h200: { tp: 1, ep: 0, bf16: true, fp8: true },
            b200: { tp: 1, ep: 0, bf16: true, fp8: true }
        },
        '4b': {
            baseName: '4B',
            isMOE: false,
            h100: { tp: 1, ep: 0, bf16: true, fp8: true },
            h200: { tp: 1, ep: 0, bf16: true, fp8: true },
            b200: { tp: 1, ep: 0, bf16: true, fp8: true }
        },
        '1.7b': {
            baseName: '1.7B',
            isMOE: false,
            h100: { tp: 1, ep: 0, bf16: true, fp8: true },
            h200: { tp: 1, ep: 0, bf16: true, fp8: true },
            b200: { tp: 1, ep: 0, bf16: true, fp8: true }
        },
        '0.6b': {
            baseName: '0.6B',
            isMOE: false,
            h100: { tp: 1, ep: 0, bf16: true, fp8: true },
            h200: { tp: 1, ep: 0, bf16: true, fp8: true },
            b200: { tp: 1, ep: 0, bf16: true, fp8: true }
        }
    },
  
    specialCommands: {
        'h100-235b-bf16-instruct': '# Error: Model is too large, cannot fit into 8*H100\n# Please use H200 (141GB) or select FP8 quantization',
        'h100-235b-bf16-thinking': '# Error: Model is too large, cannot fit into 8*H100\n# Please use H200 (141GB) or select FP8 quantization'
    },

    generateCommand: function(values) {
        const { hardware, modelsize: modelSize, quantization, thinking } = values;
        const commandKey = `${hardware}-${modelSize}-${quantization}-${thinking}`;
      
        if (this.specialCommands[commandKey]) {
            return this.specialCommands[commandKey];
        }
      
        const config = this.modelConfigs[modelSize];
        if (!config) {
            return `# Error: Unknown model size: ${modelSize}`;
        }
      
        const hwConfig = config[hardware];
        if (!hwConfig) {
            return `# Error: Unknown hardware platform: ${hardware}`;
        }
      
        const quantSuffix = quantization === 'fp8' ? '-FP8' : '';
        const thinkingSuffix = thinking === 'thinking' ? '-Thinking' : '-Instruct';
        const modelName = `Qwen/Qwen3-${config.baseName}${thinkingSuffix}${quantSuffix}`;
      
        let cmd = 'python -m sglang.launch_server \\\n';
        cmd += `  --model ${modelName}`;
      
        if (hwConfig.tp > 1) {
            cmd += ` \\\n  --tp ${hwConfig.tp}`;
        }
      
        let ep = hwConfig.ep;
        if (quantization === 'fp8' && hwConfig.tp === 8) {
            ep = 2;
        }
      
        if (ep > 0) {
            cmd += ` \\\n  --ep ${ep}`;
        }
      
        return cmd;
    }
};

function renderUI() {
    const container = document.getElementById('config-container');
    if (!container) return;
  
    let html = '';
    let index = 1;
  
    for (const [key, option] of Object.entries(CONFIG.options)) {
        html += `
<div style="background: var(--md-default-bg-color, #ffffff); padding: 16px; border-radius: 10px; margin-bottom: 12px; box-shadow: 0 3px 10px rgba(0,0,0,0.08); border: 1px solid var(--md-default-fg-color--lightest, #e0e0e0);">
    <div style="font-size: 14px; font-weight: 600; color: var(--md-default-fg-color, #2d3748); margin-bottom: 10px; display: flex; align-items: center;">
        <span style="background: #667eea; color: white; width: 22px; height: 22px; border-radius: 50%; display: inline-flex; align-items: center; justify-content: center; margin-right: 8px; font-size: 12px;">${index}</span>
        ${option.title}
    </div>
    <div style="display: flex; gap: 8px; flex-wrap: wrap;">`;
      
        if (option.type === 'text') {
            html += `
        <input type="text" id="input-${option.name}" name="${option.name}" value="${option.default || ''}" placeholder="${option.placeholder || ''}" 
               style="padding: 8px 12px; border: 1px solid var(--md-default-fg-color--lightest, #e2e8f0); border-radius: 6px; font-size: 14px; width: 100%; max-width: 300px; color: var(--md-default-fg-color, #2d3748); background: var(--md-default-bg-color, white);">`;
        } else {
            option.items.forEach(item => {
                const inputId = `${option.name}-${item.id}`;
                const checked = item.default ? 'checked' : '';
                html += `
            <input type="radio" id="${inputId}" name="${option.name}" value="${item.id}" ${checked} style="display: none;">
            <label for="${inputId}" style="padding: 8px 18px; border: 2px solid var(--md-default-fg-color--lightest, #e2e8f0); border-radius: 6px; cursor: pointer; display: inline-block; font-weight: 500; font-size: 13px; transition: all 0.3s; background: var(--md-default-bg-color, white); color: var(--md-default-fg-color, #2d3748);">${item.label}${item.subtitle ? `<br/><small style="color: var(--md-default-fg-color--light, #718096); font-size: 10px;">${item.subtitle}</small>` : ''}</label>`;
            });
        }
      
        html += `
    </div>
</div>`;
        index++;
    }
  
    html += `
<div style="background: var(--md-default-bg-color, #ffffff); padding: 16px; border-radius: 10px; box-shadow: 0 3px 10px rgba(0,0,0,0.08); border: 1px solid var(--md-default-fg-color--lightest, #e0e0e0);">
    <div style="font-size: 15px; font-weight: 600; color: var(--md-default-fg-color, #2d3748); margin-bottom: 10px;">
        Generated Command
    </div>
    <div id="command-display" style="padding: 16px; background: #2d3748; border-radius: 6px; font-family: 'Menlo', 'Monaco', 'Courier New', monospace; font-size: 13px; line-height: 1.7; color: #e2e8f0; white-space: pre-wrap; overflow-x: auto; border: none;"></div>
</div>`;
  
    container.innerHTML = html;
}

function handleChange() {
    updateQuantOptions();
    updateCommand();
    updateStyles();
}

function init() {
    const container = document.getElementById('config-container');
    if (!container) {
        setTimeout(init, 100);
        return;
    }
  
    if (container.dataset.initialized) return;
    container.dataset.initialized = 'true';
  
    renderUI();
  
    container.addEventListener('change', function(e) {
        if (e.target && e.target.matches('input[type="radio"]')) {
            handleChange();
        }
    });
  
    container.addEventListener('input', function(e) {
        if (e.target && e.target.matches('input[type="text"]')) {
            updateCommand();
        }
    });
  
    updateCommand();
    updateStyles();
}

init();
if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', init);
} else {
    init();
}
setTimeout(init, 500);

function updateQuantOptions() {
    const hardware = document.querySelector('input[name="hardware"]:checked').value;
    const modelSize = document.querySelector('input[name="modelsize"]:checked').value;
    const config = CONFIG.modelConfigs[modelSize];
  
    if (!config) return;
  
    const hwConfig = config[hardware];
    const bf16Radio = document.getElementById('quantization-bf16');
    const fp8Radio = document.getElementById('quantization-fp8');
    const bf16Label = document.querySelector('label[for="quantization-bf16"]');
    const fp8Label = document.querySelector('label[for="quantization-fp8"]');
  
    bf16Radio.disabled = !hwConfig.bf16;
    fp8Radio.disabled = !hwConfig.fp8;
  
    if (!hwConfig.bf16) {
        bf16Label.style.opacity = '0.5';
        bf16Label.style.cursor = 'not-allowed';
        if (bf16Radio.checked) {
            fp8Radio.checked = true;
        }
    } else {
        bf16Label.style.opacity = '1';
        bf16Label.style.cursor = 'pointer';
    }
  
    if (!hwConfig.fp8) {
        fp8Label.style.opacity = '0.5';
        fp8Label.style.cursor = 'not-allowed';
        if (fp8Radio.checked) {
            bf16Radio.checked = true;
        }
    } else {
        fp8Label.style.opacity = '1';
        fp8Label.style.cursor = 'pointer';
    }
  
    updateStyles();
}

function updateCommand() {
    const values = {};
    for (const [key, option] of Object.entries(CONFIG.options)) {
        if (option.type === 'text') {
            const input = document.getElementById(`input-${option.name}`);
            if (input) {
                values[option.name] = input.value;
            }
        } else {
            const selected = document.querySelector(`input[name="${option.name}"]:checked`);
            if (selected) {
                values[option.name] = selected.value;
            }
        }
    }
  
    const command = CONFIG.generateCommand(values);
    document.getElementById("command-display").textContent = command;
}

function updateStyles() {
    const labels = document.querySelectorAll('label');
  
    labels.forEach(label => {
        const input = document.querySelector(`input[id="${label.getAttribute('for')}"]`);
        if (input && input.disabled) {
            return;
        }
        label.style.backgroundColor = '';
        label.style.color = '';
        label.style.borderColor = '#ddd';
    });
  
    const checkedInputs = document.querySelectorAll('input[type="radio"]:checked:not(:disabled)');
    checkedInputs.forEach(input => {
        const label = document.querySelector(`label[for="${input.id}"]`);
        if (label) {
            label.style.backgroundColor = '#dc3545';
            label.style.color = 'white';
            label.style.borderColor = '#d55816';
        }
    });
}
*/}

### 3.2 Configuration Tips

* **Memory Management** : Set lower `--context-length` to conserve memory. A value of `128000` is sufficient for most scenarios, down from the default 262K.
* **Expert Parallelism** : SGLang supports Expert Parallelism (EP) via `--ep`, allowing experts in MoE models to be deployed on separate GPUs for better throughput. One thing to note is that, for quantized models, you need to set `--ep` to a value that satisfies the requirement: `(moe_intermediate_size / moe_tp_size) % weight_block_size_n == 0, where moe_tp_size is equal to tp_size divided by ep_size.` Note that EP may perform worse in low concurrency scenarios due to additional communication overhead. Check out [Expert Parallelism Deployment](https://github.com/sgl-project/sglang/blob/main/docs/advanced_features/expert_parallelism.md) for more details.
* **Kernel Tuning** : For MoE Triton kernel tuning on your specific hardware, refer to [fused_moe_triton](https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton).
* **Speculative Decoding**: Using Speculative Decoding for latency-sensitive scenarios.
  - `--speculative-algorithm EAGLE3`: Speculative decoding algorithm
  - `--speculative-num-steps 3`: Number of speculative verification rounds
  - `--speculative-eagle-topk 1`: Top-k sampling for draft tokens
  - `--speculative-num-draft-tokens 4`: Number of draft tokens per step
  - `--speculative-draft-model-path`: The path of the draft model weights. This can be a local folder or a Hugging Face repo ID.

## 4. Model Invocation

### 4.1 Basic Usage

For basic API usage and request examples, please refer to:

- [SGLang Basic Usage Guide](https://docs.sglang.ai/basic_usage/send_request.html)
- [SGLang OpenAI Vision API Guide](https://docs.sglang.ai/basic_usage/openai_api_vision.html)

### 4.2 Advanced Usage

#### 4.2.1 Reasoning Parser

Qwen3-235B-A22B supports reasoning mode. Enable the reasoning parser during deployment to separate the thinking and content sections:

```shell
python -m sglang.launch_server \
  --model Qwen/Qwen3-235B-A22B \
  --reasoning-parser qwen3 \
  --tp 8 \
  --host 0.0.0.0 \
  --port 8000
```

**Streaming with Thinking Process:**

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="EMPTY"
)

# Enable streaming to see the thinking process in real-time
response = client.chat.completions.create(
    model="Qwen/Qwen3-235B-A22B",
    messages=[
        {"role": "user", "content": "Solve this problem step by step: What is 15% of 240?"}
    ],
    temperature=0.7,
    max_tokens=2048,
    stream=True
)

# Process the stream
has_thinking = False
has_answer = False
thinking_started = False

for chunk in response:
    if chunk.choices and len(chunk.choices) > 0:
        delta = chunk.choices[0].delta
  
        # Print thinking process
        if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
            if not thinking_started:
                print("=============== Thinking =================", flush=True)
                thinking_started = True
            has_thinking = True
            print(delta.reasoning_content, end="", flush=True)
  
        # Print answer content
        if delta.content:
            # Close thinking section and add content header
            if has_thinking and not has_answer:
                print("\n=============== Content =================", flush=True)
                has_answer = True
            print(delta.content, end="", flush=True)

print()
```

**Output Example:**

````
=============== Thinking =================

Okay, so I need to figure out what 15% of 240 is. Hmm, percentages can sometimes trip me up, but I think I remember some basics. Let me start by recalling that "percent" means "per hundred," so 15% is the same as 15 per 100, or 15/100. So, maybe I can convert 15% into a decimal first? Yeah, I think that's a common method. 
...
So conclusion: The answer is 36.

=============== Content =================


To determine what 15% of 240 is, we can follow a systematic approach that involves converting the percentage to a decimal and then performing multiplication. Here's a step-by-step breakdown of the solution:

....

### Final Answer:

$$
\boxed{36}
$$

Thus, 15% of 240 is **36**.
````

**Note:** The reasoning parser captures the model's step-by-step thinking process, allowing you to see how the model arrives at its conclusions.

#### 4.2.3 Tool Calling

Qwen3 supports tool calling capabilities. Enable the tool call parser:

```shell
python -m sglang.launch_server \
  --model Qwen/Qwen3-235B-A22B \
  --reasoning-parser qwen3 \
  --tool-call-parser qwen3 \
  --tp 8 \
  --host 0.0.0.0 \
  --port 8000
```

**Python Example (with Thinking Process):**

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="EMPTY"
)

# Define available tools
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get the current weather for a location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city name"
                    },
                    "unit": {
                        "type": "string",
                        "enum": ["celsius", "fahrenheit"],
                        "description": "Temperature unit"
                    }
                },
                "required": ["location"]
            }
        }
    }
]

# Make request with streaming to see thinking process
response = client.chat.completions.create(
    model="Qwen/Qwen3-235B-A22B",
    messages=[
        {"role": "user", "content": "What's the weather in Beijing?"}
    ],
    tools=tools,
    temperature=0.7,
    stream=True
)

# Process streaming response
thinking_started = False
has_thinking = False
tool_calls_accumulator = {}

for chunk in response:
    if chunk.choices and len(chunk.choices) > 0:
        delta = chunk.choices[0].delta
  
        # Print thinking process
        if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
            if not thinking_started:
                print("=============== Thinking =================", flush=True)
                thinking_started = True
            has_thinking = True
            print(delta.reasoning_content, end="", flush=True)
  
        # Accumulate tool calls
        if hasattr(delta, 'tool_calls') and delta.tool_calls:
            # Close thinking section if needed
            if has_thinking and thinking_started:
                print("\n=============== Content =================\n", flush=True)
                thinking_started = False
  
            for tool_call in delta.tool_calls:
                index = tool_call.index
                if index not in tool_calls_accumulator:
                    tool_calls_accumulator[index] = {
                        'name': None,
                        'arguments': ''
                    }
  
                if tool_call.function:
                    if tool_call.function.name:
                        tool_calls_accumulator[index]['name'] = tool_call.function.name
                    if tool_call.function.arguments:
                        tool_calls_accumulator[index]['arguments'] += tool_call.function.arguments
  
        # Print content
        if delta.content:
            print(delta.content, end="", flush=True)

# Print accumulated tool calls
for index, tool_call in sorted(tool_calls_accumulator.items()):
    print(f"ðŸ”§ Tool Call: {tool_call['name']}")
    print(f"   Arguments: {tool_call['arguments']}")

print()
```

**Output Example:**

```
=============== Thinking =================

Okay, the user is asking for the weather in Beijing. Let me check the tools available. There's a function called get_weather that takes location and unit parameters. The location is required, so I need to specify Beijing as the location. The unit is optional and can be either celsius or fahrenheit. Since the user didn't specify the unit, maybe I should default to a common one. In China, they usually use celsius, so I'll set unit to celsius. I'll call the get_weather function with location: Beijing and unit: celsius. That should get the current weather for them.



=============== Content =================

ðŸ”§ Tool Call: get_weather
   Arguments: {"location": "Beijing", "unit": "celsius"}
```

**Note:**

- The reasoning parser shows how the model decides to use a tool
- Tool calls are clearly marked with the function name and arguments
- You can then execute the function and send the result back to continue the conversation

**Handling Tool Call Results:**

```python
# After getting the tool call, execute the function
def get_weather(location, unit="celsius"):
    # Your actual weather API call here
    return f"The weather in {location} is 22Â°{unit[0].upper()} and sunny."

# Send tool result back to the model
messages = [
    {"role": "user", "content": "What's the weather in Beijing?"},
    {
        "role": "assistant",
        "content": None,
        "tool_calls": [{
            "id": "call_123",
            "type": "function",
            "function": {
                "name": "get_weather",
                "arguments": '{"location": "Beijing", "unit": "celsius"}'
            }
        }]
    },
    {
        "role": "tool",
        "tool_call_id": "call_123",
        "content": get_weather("Beijing", "celsius")
    }
]

final_response = client.chat.completions.create(
    model="Qwen/Qwen3-235B-A22B",
    messages=messages,
    temperature=0.7
)

print(final_response.choices[0].message.content)
# Output: "The current weather in Beijing is **22Â°C** and **sunny**. A perfect day to enjoy outdoor activities! ðŸŒž"
```

## 5. Benchmark

### 5.1 Speed Benchmark

**Test Environment:**

- Hardware: NVIDIA B200 GPU (8x)
- Model: Qwen3-235B-A22B
- Tensor Parallelism: 8
- sglang version: 0.5.6

We use SGLang's built-in benchmarking tool to conduct performance evaluation on the [ShareGPT_Vicuna_unfiltered](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered) dataset. This dataset contains real conversation data and can better reflect performance in actual use scenarios.

#### 5.1.1 Standard Scenario Benchmark

- Model Deployment Command:

```shell
python -m sglang.launch_server \
  --model Qwen/Qwen3-235B-A22B \
  --tp 8
```

##### 5.1.1.1 Low Concurrency

- Benchmark Command:

```shell
python3 -m sglang.bench_serving \
  --backend sglang \
  --model Qwen/Qwen3-235B-A22B \
  --dataset-name random \
  --random-input-len 1000 \
  --random-output-len 1000 \
  --num-prompts 10 \
  --max-concurrency 1
```

- Test Results:

```
============ Serving Benchmark Result ============
Backend:                                 sglang  
Traffic request rate:                    inf     
Max request concurrency:                 1       
Successful requests:                     10      
Benchmark duration (s):                  44.16   
Total input tokens:                      6101    
Total input text tokens:                 6101    
Total input vision tokens:               0       
Total generated tokens:                  4210    
Total generated tokens (retokenized):    4210    
Request throughput (req/s):              0.23    
Input token throughput (tok/s):          138.14  
Output token throughput (tok/s):         95.32   
Peak output token throughput (tok/s):    98.00   
Peak concurrent requests:                2       
Total token throughput (tok/s):          233.47  
Concurrency:                             1.00    
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   4414.85   
Median E2E Latency (ms):                 3499.95   
---------------Time to First Token----------------
Mean TTFT (ms):                          96.27   
Median TTFT (ms):                        65.22   
P99 TTFT (ms):                           359.71  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          10.27   
Median TPOT (ms):                        10.28   
P99 TPOT (ms):                           10.30   
---------------Inter-Token Latency----------------
Mean ITL (ms):                           10.28   
Median ITL (ms):                         10.28   
P95 ITL (ms):                            10.34   
P99 ITL (ms):                            10.40   
Max ITL (ms):                            10.76   
==================================================
```

##### 5.1.1.2 Medium Concurrency

- Benchmark Command:

```shell
python3 -m sglang.bench_serving \
  --backend sglang \
  --model Qwen/Qwen3-235B-A22B \
  --dataset-name random \
  --random-input-len 1000 \
  --random-output-len 1000 \
  --num-prompts 80 \
  --max-concurrency 16
```

- Test Results:

```
============ Serving Benchmark Result ============
Backend:                                 sglang  
Traffic request rate:                    inf     
Max request concurrency:                 16      
Successful requests:                     80      
Benchmark duration (s):                  48.01   
Total input tokens:                      39668   
Total input text tokens:                 39668   
Total input vision tokens:               0       
Total generated tokens:                  40725   
Total generated tokens (retokenized):    40725   
Request throughput (req/s):              1.67    
Input token throughput (tok/s):          826.27  
Output token throughput (tok/s):         848.29  
Peak output token throughput (tok/s):    1152.00   
Peak concurrent requests:                20      
Total token throughput (tok/s):          1674.56   
Concurrency:                             13.58   
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   8150.37   
Median E2E Latency (ms):                 8608.58   
---------------Time to First Token----------------
Mean TTFT (ms):                          219.77  
Median TTFT (ms):                        76.69   
P99 TTFT (ms):                           817.57  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          15.88   
Median TPOT (ms):                        15.72   
P99 TPOT (ms):                           21.37   
---------------Inter-Token Latency----------------
Mean ITL (ms):                           15.61   
Median ITL (ms):                         14.73   
P95 ITL (ms):                            15.50   
P99 ITL (ms):                            53.54   
Max ITL (ms):                            335.18  
==================================================
```

##### 5.1.1.3 High Concurrency

- Benchmark Command:

```shell
python3 -m sglang.bench_serving \
  --backend sglang \
  --model Qwen/Qwen3-235B-A22B \
  --dataset-name random \
  --random-input-len 1000 \
  --random-output-len 1000 \
  --num-prompts 500 \
  --max-concurrency 100
```

- Test Results:

```
============ Serving Benchmark Result ============
Backend:                                 sglang  
Traffic request rate:                    inf     
Max request concurrency:                 100     
Successful requests:                     500     
Benchmark duration (s):                  86.32   
Total input tokens:                      249831  
Total input text tokens:                 249831  
Total input vision tokens:               0       
Total generated tokens:                  252162  
Total generated tokens (retokenized):    252131  
Request throughput (req/s):              5.79    
Input token throughput (tok/s):          2894.15   
Output token throughput (tok/s):         2921.15   
Peak output token throughput (tok/s):    4200.00   
Peak concurrent requests:                111     
Total token throughput (tok/s):          5815.30   
Concurrency:                             89.76   
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   15496.34  
Median E2E Latency (ms):                 15116.24  
---------------Time to First Token----------------
Mean TTFT (ms):                          325.26  
Median TTFT (ms):                        98.25   
P99 TTFT (ms):                           1484.77   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          30.52   
Median TPOT (ms):                        31.62   
P99 TPOT (ms):                           35.22   
---------------Inter-Token Latency----------------
Mean ITL (ms):                           30.14   
Median ITL (ms):                         23.78   
P95 ITL (ms):                            63.71   
P99 ITL (ms):                            80.10   
Max ITL (ms):                            1019.75   
==================================================
```

#### 5.1.2 Reasoning Scenario Benchmark

- Model Deployment Command:

```shell
python -m sglang.launch_server \
  --model Qwen/Qwen3-235B-A22B \
  --tp 8
```

##### 5.1.2.1 Low Concurrency

- Benchmark Command:

```
python3 -m sglang.bench_serving \
  --backend sglang \
  --model Qwen/Qwen3-235B-A22B \
  --dataset-name random \
  --random-input-len 1000 \
  --random-output-len 8000 \
  --num-prompts 10 \
  --max-concurrency 1
```

- Test Results:

```
============ Serving Benchmark Result ============
Backend:                                 sglang  
Traffic request rate:                    inf     
Max request concurrency:                 1       
Successful requests:                     10      
Benchmark duration (s):                  461.12  
Total input tokens:                      6101    
Total input text tokens:                 6101    
Total input vision tokens:               0       
Total generated tokens:                  44452   
Total generated tokens (retokenized):    44419   
Request throughput (req/s):              0.02    
Input token throughput (tok/s):          13.23   
Output token throughput (tok/s):         96.40   
Peak output token throughput (tok/s):    98.00   
Peak concurrent requests:                2       
Total token throughput (tok/s):          109.63  
Concurrency:                             1.00    
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   46110.08  
Median E2E Latency (ms):                 49701.17  
---------------Time to First Token----------------
Mean TTFT (ms):                          91.23   
Median TTFT (ms):                        64.99   
P99 TTFT (ms):                           313.50  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          10.33   
Median TPOT (ms):                        10.34   
P99 TPOT (ms):                           10.38   
---------------Inter-Token Latency----------------
Mean ITL (ms):                           10.35   
Median ITL (ms):                         10.35   
P95 ITL (ms):                            10.45   
P99 ITL (ms):                            10.48   
Max ITL (ms):                            11.68   
==================================================
```

##### 5.1.2.2 Medium Concurrency

- Benchmark Command:

```
python3 -m sglang.bench_serving \
  --backend sglang \
  --model Qwen/Qwen3-235B-A22B \
  --dataset-name random \
  --random-input-len 1000 \
  --random-output-len 8000 \
  --num-prompts 80 \
  --max-concurrency 16
```

- Test Results:

```
============ Serving Benchmark Result ============
Backend:                                 sglang  
Traffic request rate:                    inf     
Max request concurrency:                 16      
Successful requests:                     80      
Benchmark duration (s):                  340.59  
Total input tokens:                      39668   
Total input text tokens:                 39668   
Total input vision tokens:               0       
Total generated tokens:                  318226  
Total generated tokens (retokenized):    318120  
Request throughput (req/s):              0.23    
Input token throughput (tok/s):          116.47  
Output token throughput (tok/s):         934.33  
Peak output token throughput (tok/s):    1136.00   
Peak concurrent requests:                18      
Total token throughput (tok/s):          1050.80   
Concurrency:                             13.84   
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   58925.28  
Median E2E Latency (ms):                 58874.55  
---------------Time to First Token----------------
Mean TTFT (ms):                          174.97  
Median TTFT (ms):                        72.43   
P99 TTFT (ms):                           596.87  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          14.83   
Median TPOT (ms):                        14.90   
P99 TPOT (ms):                           15.26   
---------------Inter-Token Latency----------------
Mean ITL (ms):                           14.77   
Median ITL (ms):                         14.77   
P95 ITL (ms):                            15.30   
P99 ITL (ms):                            15.67   
Max ITL (ms):                            150.37  
==================================================
```

##### 5.1.2.3 High Concurrency

- Benchmark Command:

```
python3 -m sglang.bench_serving \
  --backend sglang \
  --model Qwen/Qwen3-235B-A22B \
  --dataset-name random \
  --random-input-len 1000 \
  --random-output-len 8000 \
  --num-prompts 320 \
  --max-concurrency 64
```

- Test Results:

```
============ Serving Benchmark Result ============
Backend:                                 sglang  
Traffic request rate:                    inf     
Max request concurrency:                 64      
Successful requests:                     320     
Benchmark duration (s):                  534.33  
Total input tokens:                      158939  
Total input text tokens:                 158939  
Total input vision tokens:               0       
Total generated tokens:                  1300705   
Total generated tokens (retokenized):    1300016   
Request throughput (req/s):              0.60    
Input token throughput (tok/s):          297.45  
Output token throughput (tok/s):         2434.26   
Peak output token throughput (tok/s):    2944.00   
Peak concurrent requests:                68      
Total token throughput (tok/s):          2731.71   
Concurrency:                             56.33   
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   94064.17  
Median E2E Latency (ms):                 96651.55  
---------------Time to First Token----------------
Mean TTFT (ms):                          291.24  
Median TTFT (ms):                        86.55   
P99 TTFT (ms):                           1218.49   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          23.22   
Median TPOT (ms):                        23.49   
P99 TPOT (ms):                           24.46   
---------------Inter-Token Latency----------------
Mean ITL (ms):                           23.08   
Median ITL (ms):                         22.85   
P95 ITL (ms):                            23.59   
P99 ITL (ms):                            59.35   
Max ITL (ms):                            650.89  
==================================================
```

#### 5.1.3 Summarization Scenario Benchmark

##### 5.1.2.1 Low Concurrency

- Benchmark Command:

```
python3 -m sglang.bench_serving \
  --backend sglang \
  --model Qwen/Qwen3-235B-A22B \
  --dataset-name random \
  --random-input-len 8000 \
  --random-output-len 1000 \
  --num-prompts 10 \
  --max-concurrency 1
```

- Test Results:

```
============ Serving Benchmark Result ============
Backend:                                 sglang  
Traffic request rate:                    inf     
Max request concurrency:                 1       
Successful requests:                     10      
Benchmark duration (s):                  45.24   
Total input tokens:                      41941   
Total input text tokens:                 41941   
Total input vision tokens:               0       
Total generated tokens:                  4210    
Total generated tokens (retokenized):    4210    
Request throughput (req/s):              0.22    
Input token throughput (tok/s):          927.04  
Output token throughput (tok/s):         93.06   
Peak output token throughput (tok/s):    98.00   
Peak concurrent requests:                2       
Total token throughput (tok/s):          1020.09   
Concurrency:                             1.00    
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   4522.50   
Median E2E Latency (ms):                 3640.13   
---------------Time to First Token----------------
Mean TTFT (ms):                          159.87  
Median TTFT (ms):                        147.35  
P99 TTFT (ms):                           334.09  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          10.38   
Median TPOT (ms):                        10.39   
P99 TPOT (ms):                           10.46   
---------------Inter-Token Latency----------------
Mean ITL (ms):                           10.39   
Median ITL (ms):                         10.39   
P95 ITL (ms):                            10.47   
P99 ITL (ms):                            10.50   
Max ITL (ms):                            11.29   
==================================================
```

##### 5.1.2.2 Medium Concurrency

- Benchmark Command:

```
python3 -m sglang.bench_serving \
  --backend sglang \
  --model Qwen/Qwen3-235B-A22B \
  --dataset-name random \
  --random-input-len 8000 \
  --random-output-len 1000 \
  --num-prompts 80 \
  --max-concurrency 16
```

- Test Results:

```
============ Serving Benchmark Result ============
Backend:                                 sglang  
Traffic request rate:                    inf     
Max request concurrency:                 16      
Successful requests:                     80      
Benchmark duration (s):                  53.01   
Total input tokens:                      300020  
Total input text tokens:                 300020  
Total input vision tokens:               0       
Total generated tokens:                  41589   
Total generated tokens (retokenized):    41579   
Request throughput (req/s):              1.51    
Input token throughput (tok/s):          5659.39   
Output token throughput (tok/s):         784.51  
Peak output token throughput (tok/s):    1152.00   
Peak concurrent requests:                20      
Total token throughput (tok/s):          6443.90   
Concurrency:                             13.93   
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   9234.00   
Median E2E Latency (ms):                 9575.11   
---------------Time to First Token----------------
Mean TTFT (ms):                          388.55  
Median TTFT (ms):                        182.46  
P99 TTFT (ms):                           1926.02   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          17.46   
Median TPOT (ms):                        17.41   
P99 TPOT (ms):                           28.28   
---------------Inter-Token Latency----------------
Mean ITL (ms):                           17.05   
Median ITL (ms):                         14.28   
P95 ITL (ms):                            14.88   
P99 ITL (ms):                            100.67  
Max ITL (ms):                            1431.38   
==================================================
```

##### 5.1.2.3 High Concurrency

- Benchmark Command:

```
python3 -m sglang.bench_serving \
  --backend sglang \
  --model Qwen/Qwen3-235B-A22B \
  --dataset-name random \
  --random-input-len 8000 \
  --random-output-len 1000 \
  --num-prompts 320 \
  --max-concurrency 64
```

- Test Results:

```
============ Serving Benchmark Result ============
Backend:                                 sglang  
Traffic request rate:                    inf     
Max request concurrency:                 64      
Successful requests:                     320     
Benchmark duration (s):                  103.23  
Total input tokens:                      1273893   
Total input text tokens:                 1273893   
Total input vision tokens:               0       
Total generated tokens:                  169680  
Total generated tokens (retokenized):    169679  
Request throughput (req/s):              3.10    
Input token throughput (tok/s):          12340.40  
Output token throughput (tok/s):         1643.72   
Peak output token throughput (tok/s):    2750.00   
Peak concurrent requests:                71      
Total token throughput (tok/s):          13984.11  
Concurrency:                             59.06   
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   19051.11  
Median E2E Latency (ms):                 18542.35  
---------------Time to First Token----------------
Mean TTFT (ms):                          896.85  
Median TTFT (ms):                        215.93  
P99 TTFT (ms):                           6302.90   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          35.36   
Median TPOT (ms):                        36.07   
P99 TPOT (ms):                           57.59   
---------------Inter-Token Latency----------------
Mean ITL (ms):                           34.30   
Median ITL (ms):                         22.38   
P95 ITL (ms):                            110.00  
P99 ITL (ms):                            165.88  
Max ITL (ms):                            5855.77   
==================================================
```

### 5.2 Accuracy Benchmark

#### 5.2.1 GSM8K Benchmark

- **Benchmark Command:**

```shell
python3 -m sglang.test.few_shot_gsm8k --num-questions 200
```

- **Results**:

  - Qwen/Qwen3-235B-A22B
    ```
    Accuracy: 0.945
    Invalid: 0.000
    Latency: 11.980 s
    Output throughput: 2358.105 token/s
    ```
