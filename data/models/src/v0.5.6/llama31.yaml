# Llama 3.1 Model Configurations (Simplified Format)
# This file is compiled to data/models/generated/llama31.yaml

vendor: meta-llama

defaults:
  hardware:
    H100: { tp: 4 }
    H200: { tp: 4 }
  configurations:
    - name: default
      nodes: single
      optimization: balanced
    - name: throughput-optimized
      nodes: single
      optimization: high-throughput
      enable_dp_attention: true
      extra_args:
        - --mem-fraction-static
        - "0.85"
    - name: latency-optimized
      nodes: single
      optimization: low-latency
      extra_args:
        - --speculative-algorithm
        - EAGLE
        - --speculative-num-steps
        - "3"
        - --speculative-eagle-topk
        - "1"
        - --speculative-num-draft-tokens
        - "4"
        - --disable-shared-experts-fusion
        - --max-running-requests
        - "64"
        - --mem-fraction-static
        - "0.85"
        - --kv-cache-dtype
        - fp8_e4m3
        - --context-length
        - "32768"
        - --quantization
        - fp8

families:
  - name: Llama-3.1
    description: Llama 3.1 70B Instruct model
    llm:
      thinking_capability: non_thinking

    models:
      - name: Llama-3.1-70B-Instruct
        quantization: bf16
