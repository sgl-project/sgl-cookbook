# Llama 4-Scout Model Configurations (Simplified Format)
# This file is compiled to data/models/generated/llama4scout.yaml

vendor: meta-llama

defaults:
  hardware:
    H100: { tp: 8 }
    H200: { tp: 8 }
    B200: { tp: 8 }
  configurations:
    - name: default
      nodes: single
      optimization: balanced
      extra_args:
        - --enable-multimodal
        - --context-length
        - "65536"
        - --dtype
        - bfloat16
        - --trust-remote-code
    - name: speculative-eagle3
      nodes: single
      optimization: low-latency
      extra_args:
        - --enable-multimodal
        - --context-length
        - "65536"
        - --dtype
        - bfloat16
        - --trust-remote-code
        - --speculative-algorithm
        - EAGLE3
        - --speculative-draft-model-path
        - lmsys/sglang-EAGLE3-Llama-4-Scout-17B-16E-Instruct-v1
        - --speculative-num-steps
        - "3"
        - --speculative-eagle-topk
        - "1"
        - --speculative-num-draft-tokens
        - "4"
        - --mem-fraction-static
        - "0.75"
        - --cuda-graph-max-bs
        - "2"

families:
  - name: Llama-4-Scout
    description: Llama 4-Scout multimodal instruction model
    llm:
      thinking_capability: non_thinking
      tool_parser: pythonic

    models:
      # Llama-4-Scout-17B-16E-Instruct - single model, different quantizations
      - name: Llama-4-Scout-17B-16E-Instruct
        model_path: meta-llama/Llama-4-Scout-17B-16E-Instruct
        quantization: bf16

      - name: Llama-4-Scout-17B-16E-Instruct-FP8
        model_path: meta-llama/Llama-4-Scout-17B-16E-Instruct
        quantization: fp8
